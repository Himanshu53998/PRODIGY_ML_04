{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPHUo+0UiL5V+EwjaFAOSge",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Himanshu53998/PRODIGY_ML_04/blob/main/ML_Task4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Hand Gesture Recognition - Task 4 (CNN Model).ipynb\n",
        "\n",
        "CNN model for better accuracy on LeapGestRecog dataset\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 1: SETUP AND LOAD DATASET\n",
        "# ============================================================================\n",
        "print(\"üöÄ Setting up CNN for Hand Gesture Recognition...\")\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q tensorflow opencv-python scikit-learn matplotlib seaborn\n",
        "\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "import random\n",
        "from google.colab import files\n",
        "\n",
        "# TensorFlow/Keras for CNN\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Sklearn for preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")"
      ],
      "metadata": {
        "id": "y0zgBO0lnjJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 2: LOAD YOUR DATASET FROM DRIVE\n",
        "# ============================================================================\n",
        "print(\"üìÇ Loading dataset from your Google Drive...\")\n",
        "\n",
        "# Path to your dataset\n",
        "dataset_path = '/content/drive/MyDrive/leapGestRecog'\n",
        "\n",
        "# Verify dataset exists\n",
        "if not os.path.exists(dataset_path):\n",
        "    print(f\"‚ùå Dataset not found at: {dataset_path}\")\n",
        "    print(\"Please check the path and update if needed\")\n",
        "    # Try to find it\n",
        "    !find /content/drive/MyDrive -name \"*leap*\" -type d 2>/dev/null\n",
        "else:\n",
        "    print(f\"‚úÖ Dataset found at: {dataset_path}\")\n",
        "\n",
        "# List all subjects and gestures\n",
        "subjects = sorted([s for s in os.listdir(dataset_path) if s.isdigit()])\n",
        "first_subject = os.path.join(dataset_path, subjects[0])\n",
        "gesture_classes = sorted(os.listdir(first_subject))\n",
        "\n",
        "print(f\"\\nüìä Dataset Information:\")\n",
        "print(f\"   Subjects: {len(subjects)} ({subjects[:3]}...)\")\n",
        "print(f\"   Gesture Classes: {len(gesture_classes)}\")\n",
        "print(f\"   Gestures: {gesture_classes}\")\n",
        "\n",
        "# Show sample counts\n",
        "sample_path = os.path.join(dataset_path, subjects[0], gesture_classes[0])\n",
        "sample_images = [f for f in os.listdir(sample_path) if f.endswith('.png')]\n",
        "print(f\"   Images per gesture per subject: ~{len(sample_images)}\")\n",
        "print(f\"   Total estimated images: {len(subjects) * len(gesture_classes) * len(sample_images)}\")"
      ],
      "metadata": {
        "id": "1WaNSRkrFPZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 3: ENHANCED DATA LOADER FOR CNN\n",
        "# ============================================================================\n",
        "print(\"\\nüì• Loading and preprocessing images for CNN...\")\n",
        "\n",
        "def load_data_for_cnn(num_gestures=10, samples_per_gesture=400, img_size=64, augment=False):\n",
        "    \"\"\"\n",
        "    Load data optimized for CNN training\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    # Use selected gestures\n",
        "    selected_gestures = gesture_classes[:num_gestures]\n",
        "\n",
        "    print(f\"üîß Loading {num_gestures} gestures: {selected_gestures}\")\n",
        "    print(f\"   Image size: {img_size}x{img_size}\")\n",
        "    print(f\"   Samples per gesture: {samples_per_gesture}\")\n",
        "    print(f\"   Data augmentation: {augment}\")\n",
        "\n",
        "    # Use 8 subjects for training, 2 for validation (if we had more data)\n",
        "    train_subjects = subjects[:8]\n",
        "\n",
        "    for gesture_idx, gesture in enumerate(selected_gestures):\n",
        "        gesture_samples = 0\n",
        "\n",
        "        for subject in train_subjects:\n",
        "            gesture_path = os.path.join(dataset_path, subject, gesture)\n",
        "\n",
        "            if not os.path.exists(gesture_path):\n",
        "                continue\n",
        "\n",
        "            # Get image files\n",
        "            image_files = [f for f in os.listdir(gesture_path)\n",
        "                          if f.endswith('.png')][:samples_per_gesture//len(train_subjects)]\n",
        "\n",
        "            for img_file in image_files:\n",
        "                if gesture_samples >= samples_per_gesture:\n",
        "                    break\n",
        "\n",
        "                img_path = os.path.join(gesture_path, img_file)\n",
        "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "                if img is not None:\n",
        "                    # Resize\n",
        "                    img_resized = cv2.resize(img, (img_size, img_size))\n",
        "\n",
        "                    # Normalize to [0, 1]\n",
        "                    img_normalized = img_resized / 255.0\n",
        "\n",
        "                    # Add channel dimension for CNN\n",
        "                    img_final = np.expand_dims(img_normalized, axis=-1)\n",
        "\n",
        "                    X.append(img_final)\n",
        "                    y.append(gesture)\n",
        "                    gesture_samples += 1\n",
        "\n",
        "        print(f\"   ‚úÖ {gesture}: {gesture_samples} samples\")\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    print(f\"\\n‚úÖ Data loaded successfully!\")\n",
        "    print(f\"   X shape: {X.shape}\")\n",
        "    print(f\"   y shape: {y.shape}\")\n",
        "    print(f\"   Memory usage: {X.nbytes / (1024**3):.2f} GB\")\n",
        "\n",
        "    return X, y, selected_gestures\n",
        "\n",
        "# Load data with good parameters for CNN\n",
        "X, y, gesture_names = load_data_for_cnn(\n",
        "    num_gestures=10,           # ALL 10 gestures\n",
        "    samples_per_gesture=400,   # Good amount for training\n",
        "    img_size=64                # Good resolution for CNN\n",
        ")\n",
        "\n",
        "# Show sample images\n",
        "print(\"\\nüëÄ Sample images:\")\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "for i in range(10):\n",
        "    idx = random.randint(0, len(X)-1)\n",
        "    row = i // 5\n",
        "    col = i % 5\n",
        "    axes[row, col].imshow(X[idx].squeeze(), cmap='gray')\n",
        "    axes[row, col].set_title(f\"{y[idx]}\")\n",
        "    axes[row, col].axis('off')\n",
        "plt.suptitle(\"Sample Training Images\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NRUYAWT1FQ2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 4: PREPARE DATA FOR TRAINING\n",
        "# ============================================================================\n",
        "print(\"‚öôÔ∏è Preparing data for CNN training...\")\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# Split data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# Further split train into train/validation (80/10/10 split)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.125, random_state=42, stratify=y_train  # 0.125 * 0.8 = 0.1\n",
        ")\n",
        "\n",
        "print(\"üìä Data Split Summary:\")\n",
        "print(f\"   Training set:   {X_train.shape[0]} samples\")\n",
        "print(f\"   Validation set: {X_val.shape[0]} samples\")\n",
        "print(f\"   Test set:       {X_test.shape[0]} samples\")\n",
        "print(f\"   Input shape:    {X_train.shape[1:]}\")\n",
        "print(f\"   Number of classes: {len(gesture_names)}\")\n",
        "print(f\"   Classes: {gesture_names}\")\n",
        "\n",
        "# Class distribution\n",
        "print(\"\\nüìà Class Distribution:\")\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "for cls, count in zip(unique, counts):\n",
        "    print(f\"   Class {cls} ({le.inverse_transform([cls])[0]}): {count} samples\")"
      ],
      "metadata": {
        "id": "Mwz6tTA6FUMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 5: BUILD CNN MODEL\n",
        "# ============================================================================\n",
        "print(\"üèóÔ∏è Building CNN Model Architecture...\")\n",
        "\n",
        "# Model parameters\n",
        "input_shape = X_train.shape[1:]  # (64, 64, 1)\n",
        "num_classes = len(gesture_names)\n",
        "\n",
        "# Clear any existing models\n",
        "keras.backend.clear_session()\n",
        "\n",
        "# Build the CNN model\n",
        "model = models.Sequential([\n",
        "    # First Convolutional Block\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Second Convolutional Block\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Third Convolutional Block\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Flatten and Dense layers\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    # Output layer\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Visualize model architecture\n",
        "keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "BO2zkpNNFfPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 6: COMPILE AND TRAIN CNN\n",
        "# ============================================================================\n",
        "print(\"üéØ Compiling and Training CNN Model...\")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Callbacks for better training\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=0.00001,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"‚ö° Starting training...\")\n",
        "print(f\"   Epochs: 30\")\n",
        "print(f\"   Batch size: 32\")\n",
        "print(f\"   Training samples: {X_train.shape[0]}\")\n",
        "print(f\"   Validation samples: {X_val.shape[0]}\")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training completed!\")"
      ],
      "metadata": {
        "id": "IuWwl_8GFiED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 7: EVALUATE MODEL\n",
        "# ============================================================================\n",
        "print(\"üìä Evaluating Model Performance...\")\n",
        "\n",
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Accuracy plot\n",
        "axes[0].plot(history.history['accuracy'], label='Training Accuracy')\n",
        "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('Training and Validation Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Loss plot\n",
        "axes[1].plot(history.history['loss'], label='Training Loss')\n",
        "axes[1].plot(history.history['val_loss'], label='Validation Loss')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].set_title('Training and Validation Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nüéØ Test Set Performance:\")\n",
        "print(f\"   Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(f\"   Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# Get predictions\n",
        "y_pred_probs = model.predict(X_test, verbose=0)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nüìã Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=gesture_names))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=gesture_names, yticklabels=gesture_names)\n",
        "plt.title(f'Confusion Matrix\\nTest Accuracy: {test_accuracy:.4f}')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Per-class accuracy\n",
        "print(\"\\nüìà Per-Class Accuracy:\")\n",
        "class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
        "for i, gesture in enumerate(gesture_names):\n",
        "    print(f\"   {gesture}: {class_accuracies[i]:.3f}\")"
      ],
      "metadata": {
        "id": "9Oo6h9BpR_EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 8: TEST WITH RANDOM DATASET IMAGES\n",
        "# ============================================================================\n",
        "print(\"üß™ Testing Model with Random Dataset Images...\")\n",
        "\n",
        "def test_random_dataset_images(num_tests=6):\n",
        "    \"\"\"Test model with random images from the actual dataset\"\"\"\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i in range(num_tests):\n",
        "        # Random subject and gesture\n",
        "        subject = random.choice(subjects[8:])  # Use subjects not in training\n",
        "        gesture = random.choice(gesture_names)\n",
        "\n",
        "        # Get random image\n",
        "        gesture_path = os.path.join(dataset_path, subject, gesture)\n",
        "        image_files = [f for f in os.listdir(gesture_path) if f.endswith('.png')]\n",
        "\n",
        "        if not image_files:\n",
        "            continue\n",
        "\n",
        "        image_file = random.choice(image_files)\n",
        "        image_path = os.path.join(gesture_path, image_file)\n",
        "\n",
        "        # Load and process image\n",
        "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "        img_resized = cv2.resize(img, (64, 64)) / 255.0\n",
        "        img_input = np.expand_dims(img_resized, axis=(0, -1))\n",
        "\n",
        "        # Predict\n",
        "        predictions = model.predict(img_input, verbose=0)[0]\n",
        "        predicted_idx = np.argmax(predictions)\n",
        "        predicted_gesture = gesture_names[predicted_idx]\n",
        "        confidence = predictions[predicted_idx]\n",
        "\n",
        "        # Get top 3 predictions\n",
        "        top_3_indices = np.argsort(predictions)[-3:][::-1]\n",
        "        top_3_predictions = [(gesture_names[idx], predictions[idx]) for idx in top_3_indices]\n",
        "\n",
        "        results.append({\n",
        "            'true_gesture': gesture,\n",
        "            'predicted_gesture': predicted_gesture,\n",
        "            'confidence': confidence,\n",
        "            'top_3': top_3_predictions,\n",
        "            'image': img,\n",
        "            'correct': gesture == predicted_gesture\n",
        "        })\n",
        "\n",
        "    # Display results\n",
        "    num_results = len(results)\n",
        "    cols = min(3, num_results)\n",
        "    rows = (num_results + cols - 1) // cols\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
        "\n",
        "    for i, result in enumerate(results):\n",
        "        row = i // cols\n",
        "        col = i % cols\n",
        "\n",
        "        if rows > 1:\n",
        "            ax = axes[row, col]\n",
        "        else:\n",
        "            ax = axes[col]\n",
        "\n",
        "        # Display image\n",
        "        ax.imshow(result['image'], cmap='gray')\n",
        "\n",
        "        # Format title\n",
        "        color = 'green' if result['correct'] else 'red'\n",
        "        title = f\"True: {result['true_gesture']}\\n\"\n",
        "        title += f\"Pred: {result['predicted_gesture']}\\n\"\n",
        "        title += f\"Conf: {result['confidence']:.1%}\\n\"\n",
        "        title += f\"{'‚úÖ' if result['correct'] else '‚ùå'}\"\n",
        "\n",
        "        ax.set_title(title, fontsize=10, color=color)\n",
        "        ax.axis('off')\n",
        "\n",
        "    # Hide empty subplots\n",
        "    for i in range(num_results, rows*cols):\n",
        "        row = i // cols\n",
        "        col = i % cols\n",
        "        if rows > 1:\n",
        "            axes[row, col].axis('off')\n",
        "        else:\n",
        "            axes[col].axis('off')\n",
        "\n",
        "    plt.suptitle(f\"CNN Model Test Results (Accuracy on these: {sum(r['correct'] for r in results)}/{num_results})\",\n",
        "                 fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed results\n",
        "    print(\"\\nüìã Detailed Predictions:\")\n",
        "    for i, result in enumerate(results):\n",
        "        print(f\"\\nTest {i+1}:\")\n",
        "        print(f\"  True: {result['true_gesture']}\")\n",
        "        print(f\"  Predicted: {result['predicted_gesture']} ({result['confidence']:.1%})\")\n",
        "        print(f\"  Result: {'‚úÖ CORRECT' if result['correct'] else '‚ùå WRONG'}\")\n",
        "        print(f\"  Top 3 predictions:\")\n",
        "        for gesture, prob in result['top_3']:\n",
        "            print(f\"    - {gesture}: {prob:.1%}\")\n",
        "\n",
        "# Run tests\n",
        "test_random_dataset_images(6)"
      ],
      "metadata": {
        "id": "Oox_TCeYSMfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 9: UPLOAD AND PREDICT WITH CNN\n",
        "# ============================================================================\n",
        "print(\"üì§ READY FOR IMAGE UPLOAD AND PREDICTION WITH CNN!\")\n",
        "\n",
        "def predict_with_cnn():\n",
        "    \"\"\"Upload any hand image and predict with CNN\"\"\"\n",
        "\n",
        "    print(\"\\nüì§ Click 'Choose Files' to upload a hand gesture image...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"No file selected\")\n",
        "        return\n",
        "\n",
        "    # Process uploaded image\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    file_bytes = uploaded[filename]\n",
        "\n",
        "    # Convert to image\n",
        "    nparr = np.frombuffer(file_bytes, np.uint8)\n",
        "    img_color = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "\n",
        "    if img_color is None:\n",
        "        print(\"‚ùå Could not read image\")\n",
        "        return\n",
        "\n",
        "    # Convert to grayscale\n",
        "    img_gray = cv2.cvtColor(img_color, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Enhanced preprocessing for better prediction\n",
        "    # 1. Apply adaptive thresholding\n",
        "    img_thresh = cv2.adaptiveThreshold(img_gray, 255,\n",
        "                                       cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                       cv2.THRESH_BINARY_INV, 11, 2)\n",
        "\n",
        "    # 2. Find contours to locate hand\n",
        "    contours, _ = cv2.findContours(img_thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    if contours:\n",
        "        # Get the largest contour (hand)\n",
        "        largest_contour = max(contours, key=cv2.contourArea)\n",
        "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "\n",
        "        # Add padding\n",
        "        padding = 20\n",
        "        x = max(0, x - padding)\n",
        "        y = max(0, y - padding)\n",
        "        w = min(img_gray.shape[1] - x, w + 2*padding)\n",
        "        h = min(img_gray.shape[0] - y, h + 2*padding)\n",
        "\n",
        "        # Crop hand region\n",
        "        hand_roi = img_gray[y:y+h, x:x+w]\n",
        "    else:\n",
        "        # If no contours found, use center crop\n",
        "        h, w = img_gray.shape\n",
        "        size = min(h, w) // 2\n",
        "        center_x, center_y = w // 2, h // 2\n",
        "        hand_roi = img_gray[center_y-size//2:center_y+size//2,\n",
        "                           center_x-size//2:center_x+size//2]\n",
        "\n",
        "    # Resize to model input size\n",
        "    img_resized = cv2.resize(hand_roi, (64, 64))\n",
        "\n",
        "    # Normalize\n",
        "    img_normalized = img_resized / 255.0\n",
        "\n",
        "    # Prepare for model (add batch and channel dimensions)\n",
        "    img_input = np.expand_dims(img_normalized, axis=(0, -1))\n",
        "\n",
        "    # Predict with CNN\n",
        "    predictions = model.predict(img_input, verbose=0)[0]\n",
        "    predicted_idx = np.argmax(predictions)\n",
        "    predicted_gesture = gesture_names[predicted_idx]\n",
        "    confidence = predictions[predicted_idx]\n",
        "\n",
        "    # Get top 5 predictions\n",
        "    top_5_indices = np.argsort(predictions)[-5:][::-1]\n",
        "    top_5_predictions = [(gesture_names[idx], predictions[idx]) for idx in top_5_indices]\n",
        "\n",
        "    # Display results\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
        "\n",
        "    # Original color\n",
        "    axes[0, 0].imshow(cv2.cvtColor(img_color, cv2.COLOR_BGR2RGB))\n",
        "    axes[0, 0].set_title('Original Image')\n",
        "    axes[0, 0].axis('off')\n",
        "\n",
        "    # Grayscale\n",
        "    axes[0, 1].imshow(img_gray, cmap='gray')\n",
        "    axes[0, 1].set_title('Grayscale')\n",
        "    axes[0, 1].axis('off')\n",
        "\n",
        "    # Threshold\n",
        "    axes[0, 2].imshow(img_thresh, cmap='gray')\n",
        "    axes[0, 2].set_title('Threshold')\n",
        "    axes[0, 2].axis('off')\n",
        "\n",
        "    # Hand ROI\n",
        "    axes[1, 0].imshow(hand_roi, cmap='gray')\n",
        "    axes[1, 0].set_title('Hand Region')\n",
        "    axes[1, 0].axis('off')\n",
        "\n",
        "    # Processed for model\n",
        "    axes[1, 1].imshow(img_resized, cmap='gray')\n",
        "    axes[1, 1].set_title('Processed 64x64')\n",
        "    axes[1, 1].axis('off')\n",
        "\n",
        "    # Prediction chart\n",
        "    axes[1, 2].barh(range(len(top_5_predictions)),\n",
        "                    [p[1] for p in top_5_predictions])\n",
        "    axes[1, 2].set_yticks(range(len(top_5_predictions)))\n",
        "    axes[1, 2].set_yticklabels([p[0] for p in top_5_predictions])\n",
        "    axes[1, 2].set_xlabel('Confidence')\n",
        "    axes[1, 2].set_title('Top 5 Predictions')\n",
        "    axes[1, 2].set_xlim([0, 1])\n",
        "\n",
        "    # Highlight the top prediction\n",
        "    for i, (gesture, prob) in enumerate(top_5_predictions):\n",
        "        if i == 0:\n",
        "            axes[1, 2].get_yticklabels()[i].set_color('red')\n",
        "            axes[1, 2].get_yticklabels()[i].set_fontweight('bold')\n",
        "\n",
        "    plt.suptitle(f\"CNN PREDICTION: {predicted_gesture}\\nConfidence: {confidence:.1%}\",\n",
        "                 fontsize=16, color='green' if confidence > 0.8 else 'orange')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print details\n",
        "    print(f\"\\nüìä PREDICTION RESULTS:\")\n",
        "    print(f\"   File: {filename}\")\n",
        "    print(f\"   Top Prediction: {predicted_gesture} ({confidence:.1%})\")\n",
        "    print(f\"\\nüîÆ Top 5 Predictions:\")\n",
        "    for i, (gesture, prob) in enumerate(top_5_predictions):\n",
        "        prefix = \"üéØ \" if i == 0 else \"   \"\n",
        "        print(f\"{prefix}{i+1}. {gesture}: {prob:.1%}\")\n",
        "\n",
        "    # Gesture descriptions\n",
        "    gesture_descriptions = {\n",
        "        \"01_palm\": \"Open palm facing forward\",\n",
        "        \"02_l\": \"Index finger and thumb making L shape\",\n",
        "        \"03_fist\": \"Closed fist\",\n",
        "        \"04_fist_moved\": \"Fist moving sideways\",\n",
        "        \"05_thumb\": \"Thumbs up gesture\",\n",
        "        \"06_index\": \"Pointing index finger\",\n",
        "        \"07_ok\": \"OK sign (thumb and index touching)\",\n",
        "        \"08_palm_moved\": \"Palm moving/rotating\",\n",
        "        \"09_c\": \"Hand making C shape\",\n",
        "        \"10_down\": \"Hand pointing downward\"\n",
        "    }\n",
        "\n",
        "    if predicted_gesture in gesture_descriptions:\n",
        "        print(f\"\\nüìù Description: {gesture_descriptions[predicted_gesture]}\")\n",
        "\n",
        "    print(f\"\\nü§ñ Model Info: CNN trained on {len(gesture_names)} gestures\")\n",
        "    print(f\"   Test Accuracy: {test_accuracy:.1%}\")\n",
        "\n",
        "# Run the prediction\n",
        "predict_with_cnn()"
      ],
      "metadata": {
        "id": "O35gq5pESTdj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}